<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Fine-Tune GPT</title>
    <link rel="stylesheet" href="style.css">
	<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
	<script src="https://cdn.jsdelivr.net/particles.js/2.0.0/particles.min.js"></script>
	<canvas id="meteorCanvas"></canvas>
</head>
<body>
    <div id="particles-js"></div>
    <div class="wave-background">
        <div class="wave"></div>
        <div class="wave"></div>
        <div class="wave"></div>
    </div>
    <div class="particles" id="particles"></div>
	
    <header class="header">
        <h1 style="text-align: center;">Fine-Tune GPT 詳解</h1>
    </header>

    <div class="container">
        <section>
            <h3>專案背景</h3>
            <p>在這個專案中，使用transformer進行GPT-2的微調，讓它能夠精準整理並回答我準備的問答資料。這裡是以Google Colab的T4 GPU執行運算，並再Drive中保存模型，以利未來修改。</p>
            <br>
			
            <h3>數據集介紹</h3>
            <p>我使用80個有關統計與機器學習的問答資料集，例如:</p>
            <br>
            <ul>
                <h4><strong>1. Q: "What is linear regression?" :</strong></h4>
				<br>
				<li>A: "Linear regression is a statistical method used to model the linear relationship between independent variables (X) and dependent variables (Y). Its basic form is Y = β₀ + β₁X + ε, where β₀ is the intercept, β₁ is the slope, and ε is the error term."</li>
                <br>
				<h4><strong>2. Q: "Explain the gradient descent algorithm?" :</strong></h4>
				<br>
				<li>A: "Gradient descent is an optimization algorithm used to find the minimum of a function. It iteratively updates parameters by computing the gradient of the loss function and moving in the negative gradient direction. The learning rate controls the size of each update step."</li>
            </ul>
            <br>
			
			<h4>我們的目的是，嘗試訓練(微調)一個專業的GPT，可以回答專業的統計與機器學習的問題。</h4>
			<br>
			
			<h3>模型建立</h3>
			<br>
			<h4>以下是本篇fine-tune的重要步驟:</h4>
			<br>
            <ul>
                <li>1. 句子前處理(分為text、 instruction、output)，全文、問句、答句，</li>
				<br>
				<li>2. 對train data 和test data 進行tokenization 以及dropout(去掉一點神經元)，</li>
                <br>
				<ul>
				<li>Remark: Tokenization 的目的:</li>
				<br>
				<ul>
				<li>a. 文本 -> 數字<li>
				<br>
				<li>b. 統一長度: 填充(pad)和截斷(Truncate)<li>
				<br>
				<li>c. 保留結構: 區分問題和答案<li>
				<br>
				<li>d. 生成標籤	為模型訓練準備target<li>
				<br>
				</ul>
				</ul>
				<br>
				<li>3. 在原本的GPT2 模型中，我們另外加入Masked Self-Attention,</li>
				<br>
				<li>4. 在原本的GPT2 模型中PositionEncoding，變得也可學習，同時加入殘差連接(Residual Connections)，</li>
				<br>
				<li>5. 在原本的GPT2 模型的loss ，計算模型預測 (logits) 與真實標籤 (labels) 的差異(CrossEntropyLoss)，如果要做蒸餾，loss 也加在這，</li>
				<br>
				<li>6. 修改後的GPT2我們叫做<strong>EnhancedGPT2</strong>，</li>
				<br>
				<li>7. 分層學習率，</li>
				<br>
				<li>8. 分層梯度裁減(gradient clipping)，這是為了防止梯度爆炸，通常是對<strong>一層的所有參數</strong>的梯度進行 clip_grad_norm_，</li>
				<br>
				<li>9. 混和精度訓練(Mixed Precision Training)，簡單說，就是同時使用 float16 和 float32 精度進行訓練，float16 加速運算與節省 GPU 記憶體；float32則保留關鍵參數（比如梯度累積、損失）為保穩定性，</li>
				<br>
				<li>10. 計算在test data上的困惑度(perplexity，PPL)，PPL = exp(CrossEntropyLoss)，就是指模型的回答有多肯定，越小越肯定，通常不超過10，</li>
                <br>
				<ul>
				<li>Remark: perplexity</li>
				<br>
				<ul>
				<li>a. 只能月在NLP上，不能用在回歸或分類問題，<li>
				<br>
				<li>b. 也可以用在early stopping：當validation data的 PPL 開始上升，代表模型overfit，<li>
				<br>
				<li>c. PPL"整句回答的可信度"，用在自回歸模型(每一步都在預測下一個字)才有用，像BERT那種預測空格的，用PPL 就容易低估或高估，因為它不是生成完整的一句話，而是填空，每次只預測少量被 mask 的詞，而loss 和 perplexity 只代表模型"對填空的預測能力"，而非整體句子建模能力。<li>
				<br>
				</ul>
				</ul>
				<br>
				<br>
				<li>11. 繪製attention plot，</li>
				<br>
				<li>12. 訓練前快速驗證: 先用1個epoch和少量data檢查，</li>
				<br>
				<li>13. 最終訓練，</li>
				<br>
				<li>14. 查看訓練日誌，評估結果，以及繪製loss plot，</li>
				<br>
				<li>15. 測試輸出。</li>
				<br>
            </ul>
            <br>
			<br>
			<p>Note: 以下是常見的transformer 的名詞解釋與結構，</p>
			<figure>
			<img src="images/p36.png" alt="feature importance" width="1000" height="500">
 			<figcaption>(圖一) 基本transformer 的解釋與結構</figcaption>
			</figure>
			<br>
			<p><strong>Remark</strong>: 看起來步驟很多，但LLM 就只保留input_ids(文字->數字，像是[8241,2061,318,...])、 attention_mask(判斷哪些Position是有用的，像是[1,1,0,...]，避免關注到padding的部分)、 labels(通常跟跟input_ids差不多) 來訓練模型，原本的instruction、 output 那些只是用來形成它們，結束後沒必要保留。整體來說就是: 文字 -> 數字 -> 訓練模型 -> 數字 -> 文字。</p>
			<br>
            <h3>模型評估</h3>
            <p>我進行10個epochs，以下是train與test的loss，</p>
			<br>
			
			<figure>
			<img src="images/p8.png" alt="Cross Entropy Loss" width="450" height="300">
 			<figcaption>(圖二) Cross Entropy Loss</figcaption>
			</figure>
			<br>
			
			<h4>解釋:</h4>
			<br>
			<ul>
			<li>這個 Cross Entropy Loss 可以知道模型的loss收斂是平穩地下降，但6之後下降速度變慢，8之後loss有一點點上升，並且有輕微的overfit，這點從train和test的Gap的差異就看得出來，以平坦程度來看，loss收斂也不完全，epoch應該可以再多，我想至少也要30以上。</li>
			</ul>
            <br>
			
			<h4>問題與處理方案:</h4>
			<ul>
			    <li>1. 後期收斂不足(6之後):</li>
			    <ul>
			        <li>學習率過高，導致收斂不好(像是一個點在最佳解旁邊繞)。</li>
			    </ul>
			<br>
			    <li>2. 稍微overfit(8之後):</li>
				    <ul>
			            <li>在eval loss > train loss + 0.3就應該出手調整。</li>
					</ul>
			</ul>
			<br>
			
			<h3>生成答句</h3>
			<ul>
                <h4>Input: </h4>
				<ul>
				    <li>What is linear regression?</li>
				</ul>
				<br>
				<h4>Output: </h4>
				<ul>
				    <li>Answer: Linear regression is the method for estimating linear trends in the mean of continuous data (logarithmistic regression). Linear regression is based on the idea that there is a continuous variable and an error term. For linear regression, the error term is a function of the number of continuous variables. For logarithmic regression, the error term is the difference between the mean squared and the error term.</li>
				</ul>
            </ul>
			<br>
			<h3>檢查回答內容</h3>
			<ul>
	            <li>其實回答內容還不錯，但有些錯誤，如下:</li>
				<br>
				<h4>1. logarithmistic regression(log linear regression)，但這裡是問linear regression；</h4>
				<br>
				<h4>2. 這句:"error term is a function of the number of continuous variables."，不對，error是獨立的隨機變數，且ε ~ N(0,σ²)；</h4>
				<br>
				<h4>3. error term不是mean squared and error term(MSE)，應該是:residual sum of squares(RSS)。</h4>
				<br>
			</ul>
			<br>
			
			<h3>改進方案</h3>
			<p>1. 多加一點<strong>"錯誤 vs 正確"</strong>的對照版本在文本中，比如:</p>
			<br>
			<ul>
				<ul>
				<h4>QA </h4>
			        <li><strong>Input:</strong></li>
				    <ul>
				        <li>"Linear regression's error term depends on variable count".</li>
				    </ul>
				    <br>
				    <li><strong>Output:</strong></li>
				    <ul>
				        <li>"INCORRECT. Error term ε is independent noise. CORRECT: ε ~ N(0,σ²)".</li>
				    </ul>
				</ul>
				<br>
				<ul>
				<h4>QA </h4>
			        <li><strong>Input:</strong></li>
			    	<ul>
				        <li>"Logarithmistic regression".</li>
				    </ul>
				    <br>
				    <li><strong>Output:</strong></li>
				    <ul>
				        <li>"INCORRECT. Use 'logistic regression' for classification; and 'log-linear regression' for exponential relationships".</li>
				    </ul>
				</ul>
			</ul>
			<br>
			<p>2. 又或者，對生成參數強化，比如: 降低隨機性(temperature)，或者建立一些生成的限制(generation_args)，比如: "forced_words"和"bad_word"。</p>
			<br>
			<p>3. 添加更多重複的topics，但以不同的內容回答，這樣在驗證時才能保證有學到該問題的訊息。比如說: linear_regression的definitions有least Squared estimate、 Y=βX+ε；applications有house predict、 trend analysis之類的，
			      每個核心概念提供好幾種種不同描述方式,這樣有助於模型的泛化能力提升，再藉由損失函數，給同概念的答案向量相似度(cosine_similarity...)懲罰，使得同概念輸出更穩定。</p>
			<br>
			
            <h3>結論</h3>
            <p>這個專案，用一筆簡單的資料，展示了如何微調一個GPT-2模型，讓它可以回答更專業的問題。以這個問題來說，我們仍需要'術語校對'和'數學嚴謹性強化'，進一步提升回答品質。我認為應該優先修正訓練數據中的技術表答(錯誤 vs 正確的對照樣本)。</p>
	    <br>
	    <p>另外，其實這裡有個小問題，就是我們應該需要更多樣本，且增加具有重複效果，但不同方式描述的語句，不然輸出的語句即使與輸入的完全相同，那也沒關係的話，那其實就失去"驗證"的意義；此外，被分類到test 的部份，模型也學不到它們的訊息。</p>
        </section>
    </div>

    <footer class="footer">
        <div class="container">
            <p>&copy; 2025 個人作品集. All rights reserved.</p>
        </div>
    </footer>
	
	<script src="script.js"></script>
</body>
</html>
